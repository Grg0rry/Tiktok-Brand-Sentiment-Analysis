{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "import networkx as nx\n",
    "from nltk import FreqDist, bigrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv(\"TikTokUserTweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(string_df):\n",
    "    # punctuation list created\n",
    "    punc= '''!()-[]{};:'\"\\,=<>./?@#$%^&*_~'''+'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'\n",
    "\n",
    "    # compress all rows into 1 string only in 'string' variable\n",
    "    twstring =''.join([str(item) for item in string_df.tolist()])\n",
    "    for letter in twstring:\n",
    "        if letter in punc:\n",
    "            twstring = twstring.replace(letter,\"\")\n",
    "\n",
    "    # generated wordcloud\n",
    "    tweets_wordcloud = WordCloud(width=400,height=400,background_color='white',min_font_size=10,collocations=False).generate(twstring)\n",
    "    plt.imshow(tweets_wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 1: To identify trending or most commonly used terms or topics on all tweets on TikTok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot wordcloud on all tweets\n",
    "plot_wordcloud(data['Tweets_Cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Modelling\n",
    "cleaned_tokens = data['Tweets_Cleaned'].tolist()\n",
    "\n",
    "id2word=corpora.Dictionary(cleaned_tokens)\n",
    "print(id2word.token2id)\n",
    "\n",
    "id2word.filter_extremes(no_below=30,no_above=30)\n",
    "print(id2word.token2id)\n",
    "\n",
    "corpus=[id2word.doc2bow(text) for text in cleaned_tokens]\n",
    "\n",
    "ldamodel=gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=id2word,passes=50,iterations=50,num_topics=7,random_state=1)\n",
    "\n",
    "ldamodel.print_topics(num_words=7)\n",
    "coherence_model_lda=CoherenceModel(model=ldamodel,texts=cleaned_tokens,dictionary=id2word,coherence='u_mass')\n",
    "\n",
    "coherence_lda=coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "print('\\nCoherence Score: ', coherence_model_lda.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display=pyLDAvis.gensim_models.prepare(ldamodel, corpus, id2word)\n",
    "\n",
    "pyLDAvis.save_html(lda_display,'lda.html')\n",
    "\n",
    "tp_list=[]\n",
    "for i in range(len(ldamodel[corpus])):\n",
    "    tp=ldamodel[corpus][i]\n",
    "    tp=sorted(tp, key=lambda tp: tp[1],reverse=True)\n",
    "    tp_list.append(tp[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dict = {\n",
    "    0: 'Studying content on TikTok and how to get popular',\n",
    "    1: 'User’s intention and possibly other people offering or requesting help or work on the platform',\n",
    "    2: 'User’s engagement and opinions',\n",
    "    3: 'Viral TikTok content, mainly about a girl doing challenge that involves music',\n",
    "    4: 'Strategizing TikTok posts with inclusion of feedback',\n",
    "    5: 'Real world and personal topics.',\n",
    "    6: 'Regarding the data access (most probably by China)'\n",
    "}\n",
    "\n",
    "tp_list = [replacement_dict.get(item, item) for item in tp_list]\n",
    "data['Topic'] = tp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in replacement_dict:\n",
    "    tweets_token = data.loc[data['Topic'] == topic]\n",
    "    \n",
    "    plot_wordcloud(tweets_token['Tweets_Cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 2a: To identify how the public feels/perceives the brand TikTok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_blob=[]\n",
    "for tweet in data['Tweets'].tolist():\n",
    "    analysis = TextBlob(tweet)\n",
    "    if analysis.sentiment.polarity==0:\n",
    "        sentiment=\"Neutral\"\n",
    "    elif analysis.sentiment.polarity>0:\n",
    "        sentiment=\"Positive\"\n",
    "    elif analysis.sentiment.polarity<0:\n",
    "        sentiment=\"Negative\"\n",
    "    text_blob.append(sentiment)\n",
    "    \n",
    "data[\"Sentiment\"]=text_blob\n",
    "\n",
    "data.groupby(by='Sentiment').mean()\n",
    "data.groupby(by='Sentiment')['Sentiment'].count()\n",
    "\n",
    "tps=data[['Tweets','Place','Sentiment']]\n",
    "tps=tps[tps.Sentiment != 'Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive sentiment wordcloud\n",
    "positive_tweets = data.loc[data['Sentiment']=='Positive']\n",
    "\n",
    "plot_wordcloud(positive_tweets['Tweets_Cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative sentiment wordcloud\n",
    "negative_tweets = data.loc[data['Sentiment']=='Negative']\n",
    "\n",
    "plot_wordcloud(negative_tweets['Tweets_Cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 2b: To identify how major countries feel/perceive this news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tps.groupby(by=['Place'])['Sentiment'].size()\n",
    "groupedforcomparison=tps.groupby(by=['Place'])['Sentiment'].size()\n",
    "\n",
    "othercountrylist=[]\n",
    "for idx, place in enumerate(groupedforcomparison):\n",
    "    if place<=15:\n",
    "        othercountrylist.append(groupedforcomparison.index[idx])\n",
    "\n",
    "len(othercountrylist)\n",
    "\n",
    "tps.loc[tps['Place'].isin(othercountrylist),'Place']='Others'\n",
    "# tps.groupby(by=['Place'])['Sentiment'].size().index[0]\n",
    "tps.groupby(by=['Place'])['Sentiment'].size()\n",
    "# tps.groupby(by=['Place','Sentiment']).size()\n",
    "\n",
    "# pbs=tps.groupby(by=['Place','Sentiment']).size().reset_index()\n",
    "# pbs=pbs.rename(columns={0:\"Count\"})\n",
    "pbs=tps.pivot_table(index='Place',columns='Sentiment',values='Sentiment', aggfunc='size')\n",
    "ax = pbs.plot(kind=\"bar\",color=[\"#ca472f\",\"#0b84a5\"], grid=True,rot=0, title=\"Country's Sentiment \\non TikTok\")\n",
    "plt.xticks(rotation=30, horizontalalignment=\"center\")\n",
    "ax.set_xlabel(\"Country\")\n",
    "ax.set_ylabel(\"count\")\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 6]\n",
    "plt.show()\n",
    "\n",
    "pbs=tps.pivot_table(index='Place',columns='Sentiment',values='Sentiment', aggfunc='size')\n",
    "pbs['Negative']=pbs['Negative'].fillna(0)\n",
    "pbs['Positive']=pbs['Positive'].fillna(0)\n",
    "pbs['Negative']=pbs['Negative']/(pbs['Negative']+pbs['Positive'])*100\n",
    "pbs['Positive']=100-pbs['Negative']\n",
    "ax = pbs.plot(kind=\"bar\",color=[\"#ca472f\",\"#0b84a5\"], grid=True,rot=0, title=\"Country's Sentiment by Percentage \\non TikTok\")\n",
    "plt.xticks(rotation=30, horizontalalignment=\"center\")\n",
    "ax.set_xlabel(\"Country\")\n",
    "ax.set_ylabel(\"count\")\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 6]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 3: To analyse how the news has impacted @TikTok_US’s engagement rates such as Likes, Retweets, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_tweets(api, username):\n",
    "    # create empty list to temporarily hold data for scraping\n",
    "    tweets=[]\n",
    "    # use for loop to keep feeding every tweets scraped using user timeline and cursor function to the empty list\n",
    "    # user timeline help to get the tweets\n",
    "    # cursor function help to look through the page\n",
    "    # need to look through the page manually without cursor function\n",
    "    \n",
    "    # can also use tweepy.Cursor(api.user_timeline,screen_name=username).pages(): to scrape by page\n",
    "    for status in tweepy.Cursor(api.user_timeline,screen_name=username).items():\n",
    "        tweets.append(status)\n",
    "    return tweets\n",
    "\n",
    "tiktoktweets=get_user_tweets(extractor, 'tiktok_us')\n",
    "print(\"Number of tweets extracted: \", len(tiktoktweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timelineTikTokProf=data[['Date','Likes_no','Retweets_no']]\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Likes_no'], label='Likes')\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Retweets_no'], label='Retweets')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"TikTok_US Engagement Over 3 Years\")\n",
    "plt.figure(figsize=(20,12)) \n",
    "plt.show()\n",
    "\n",
    "timelineTikTokProf=data[data['Date'].dt.date<datetime.strptime('2021','%Y').date()][['Date','Likes_no','Retweets_no']]\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Likes_no'], label='Likes')\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Retweets_no'], label='Retweets')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"TikTok_US Engagement in 2020\")\n",
    "plt.figure(figsize=(20,12)) \n",
    "plt.show()\n",
    "\n",
    "timelineTikTokProf=data[np.logical_and(data['Date'].dt.date<datetime.strptime('2022','%Y').date() ,\n",
    "                                       data['Date'].dt.date>=datetime.strptime('2021','%Y').date())][['Date','Likes_no','Retweets_no']]\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Likes_no'], label='Likes')\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Retweets_no'], label='Retweets')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"TikTok_US Engagement in 2021\")\n",
    "plt.figure(figsize=(20,12)) \n",
    "plt.show()\n",
    "\n",
    "timelineTikTokProf=data[np.logical_and(data['Date'].dt.date<datetime.strptime('2023','%Y').date() ,\n",
    "                                       data['Date'].dt.date>=datetime.strptime('2022','%Y').date())][['Date','Likes_no','Retweets_no']]\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Likes_no'], label='Likes')\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Retweets_no'], label='Retweets')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"TikTok_US Engagement in 2022\")\n",
    "plt.figure(figsize=(20,12)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################### Likes #####################################################################\n",
    "###### Year 2020##########\n",
    "timelineTikTokProf=data[data['Date'].dt.date<datetime.strptime('2021','%Y').date()][['Date','Likes_no']]\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Likes_no'])\n",
    "plt.figure(figsize=(20,12)) \n",
    "plt.show()\n",
    "\n",
    "most_likes_tweets= pd.DataFrame(columns=['Tweets','Tweets_ID','Date','Source','Likes_no','Retweets_no'])\n",
    "for i in range(1):\n",
    "    \n",
    "    likes_max=np.max(timelineTikTokProf['Likes_no'])\n",
    "    print(\"The \", i+1,\" tweet with the highest like count in this year is:\\n \",likes_max)\n",
    "    most_likes_tweets.append(data[data['Likes_no']==likes_max][['Tweets','Date','Likes_no']])\n",
    "    \n",
    "    timelineTikTokProf=timelineTikTokProf[timelineTikTokProf['Likes_no']!=likes_max]\n",
    "# most_likes_tweets=pd.concat(most_likes_tweets)\n",
    "\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Likes_no'])\n",
    "plt.figure(figsize=(20,12)) \n",
    "plt.show()\n",
    "\n",
    "###### Year 2021############\n",
    "timelineTikTokProf=data[np.logical_and(data['Date'].dt.date<datetime.strptime('2022','%Y').date() ,\n",
    "                                       data['Date'].dt.date>=datetime.strptime('2021','%Y').date())][['Date','Likes_no']]\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Likes_no'])\n",
    "# plt.rcParams[\"figure.figsize\"] = [20, 12]\n",
    "plt.figure(figsize=(20,12)) \n",
    "plt.show()\n",
    "\n",
    "\n",
    "# most_likes_tweets=[]\n",
    "for i in range(3):\n",
    "    \n",
    "    likes_max=np.max(timelineTikTokProf['Likes_no'])\n",
    "    print(\"The \", i+1,\" tweet with the highest like count in this year is:\\n \",likes_max)\n",
    "    most_likes_tweets.append(data[data['Likes_no']==likes_max][['Tweets','Date','Likes_no']])\n",
    "    \n",
    "    timelineTikTokProf=timelineTikTokProf[timelineTikTokProf['Likes_no']!=likes_max]\n",
    "# most_likes_tweets=pd.concat(most_likes_tweets)\n",
    "\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Likes_no'])\n",
    "plt.figure(figsize=(20,12)) \n",
    "plt.show()\n",
    "\n",
    "############ Year 2022##############\n",
    "timelineTikTokProf=data[np.logical_and(data['Date'].dt.date<datetime.strptime('2023','%Y').date() ,\n",
    "                                       data['Date'].dt.date>=datetime.strptime('2022','%Y').date())][['Date','Likes_no']]\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Likes_no'])\n",
    "# plt.rcParams[\"figure.figsize\"] = [20, 12]\n",
    "plt.figure(figsize=(20,12)) \n",
    "plt.show()\n",
    "\n",
    "# most_likes_tweets=[]\n",
    "for i in range(3):\n",
    "    \n",
    "    likes_max=np.max(timelineTikTokProf['Likes_no'])\n",
    "    print(\"The \", i+1,\" tweet with the highest like count in this year is:\\n \",likes_max)\n",
    "    most_likes_tweets.append(data[data['Likes_no']==likes_max][['Tweets','Date','Likes_no']])\n",
    "    \n",
    "    timelineTikTokProf=timelineTikTokProf[timelineTikTokProf['Likes_no']!=likes_max]\n",
    "# most_likes_tweets=pd.concat(most_likes_tweets)\n",
    "\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Likes_no'])\n",
    "plt.figure(figsize=(20,12)) \n",
    "plt.show()\n",
    "\n",
    "##################################################################### Retweets #####################################################################\n",
    "############## Year 2020############\n",
    "timelineTikTokProf=data[data['Date'].dt.date<datetime.strptime('2021','%Y').date()][['Date','Retweets_no']]\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Retweets_no'])\n",
    "plt.figure(figsize=(20,12)) \n",
    "plt.show()\n",
    "\n",
    "most_rt_tweets=pd.DataFrame(columns=['Tweets','Tweets_ID','Date','Source','Likes_no','Retweets_no'])\n",
    "for i in range(1):\n",
    "    \n",
    "    rt_max=np.max(timelineTikTokProf['Retweets_no'])\n",
    "    print(\"The \", i+1,\" tweet with the highest retweets count in this year is:\\n \",rt_max)\n",
    "    most_rt_tweets.append(data[data['Retweets_no']==rt_max][['Tweets','Date','Retweets_no']])\n",
    "    \n",
    "    timelineTikTokProf=timelineTikTokProf[timelineTikTokProf['Retweets_no']!=rt_max]\n",
    "# most_rt_tweets=pd.concat(most_rt_tweets)\n",
    "\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Retweets_no'])\n",
    "plt.figure(figsize=(20,12)) \n",
    "plt.show()\n",
    "\n",
    "\n",
    "############ Year 2021###############\n",
    "timelineTikTokProf=data[np.logical_and(data['Date'].dt.date<datetime.strptime('2022','%Y').date() ,\n",
    "                                       data['Date'].dt.date>=datetime.strptime('2021','%Y').date())][['Date','Retweets_no']]\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Retweets_no'])\n",
    "plt.figure(figsize=(20,12)) \n",
    "plt.show()\n",
    "\n",
    "# most_rt_tweets=[]\n",
    "for i in range(3):\n",
    "    \n",
    "    rt_max=np.max(timelineTikTokProf['Retweets_no'])\n",
    "    print(\"The \", i+1,\" tweet with the highest retweets count in this year is:\\n \",rt_max)\n",
    "    most_rt_tweets.append(data[data['Retweets_no']==rt_max][['Tweets','Date','Retweets_no']])\n",
    "    \n",
    "    timelineTikTokProf=timelineTikTokProf[timelineTikTokProf['Retweets_no']!=rt_max]\n",
    "# most_rt_tweets=pd.concat(most_rt_tweets)\n",
    "\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Retweets_no'])\n",
    "plt.figure(figsize=(20,12)) \n",
    "plt.show()\n",
    "\n",
    "################ Year 2022####################\n",
    "timelineTikTokProf=data[np.logical_and(data['Date'].dt.date<datetime.strptime('2023','%Y').date() ,\n",
    "                                       data['Date'].dt.date>=datetime.strptime('2022','%Y').date())][['Date','Retweets_no']]\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Retweets_no'])\n",
    "plt.figure(figsize=(20,12)) \n",
    "plt.show()\n",
    "\n",
    "# most_rt_tweets=[]\n",
    "for i in range(3):\n",
    "    \n",
    "    rt_max=np.max(timelineTikTokProf['Retweets_no'])\n",
    "    print(\"The \", i+1,\" tweet with the highest retweets count in this year is:\\n \",rt_max)\n",
    "    most_rt_tweets.append(data[data['Retweets_no']==rt_max][['Tweets','Date','Retweets_no']])\n",
    "    \n",
    "    timelineTikTokProf=timelineTikTokProf[timelineTikTokProf['Retweets_no']!=rt_max]\n",
    "# most_rt_tweets=pd.concat(most_rt_tweets)\n",
    "\n",
    "plt.plot(timelineTikTokProf['Date'],timelineTikTokProf['Retweets_no'])\n",
    "plt.figure(figsize=(20,12)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_tweetsandreplies(api,keyword,number_of_tweets):\n",
    "    # usually when searched by keywords, original tweets and retweets are given\n",
    "    # this will filter out retweets giving only the original tweets\n",
    "    new_keyword=keyword+' -filter:retweets'\n",
    "    \n",
    "    tweets=[]\n",
    "    # instead of user timeline, we use search function\n",
    "    for status in tweepy.Cursor(api.search_tweets, q=new_keyword, \n",
    "                                lang=\"en\", tweet_mode='extended', \n",
    "                                result_type='mixed').items(number_of_tweets):\n",
    "        tweets.append(status)\n",
    "        # replykey='(to:'+status.user.screen_name+') since:'+status.created_at.strftime(\"%Y-%m-%d\")\n",
    "        # for stuff in tweepy.Cursor(api.search_tweets, q=replykey, \n",
    "        #                             lang=\"en\", tweet_mode='extended').items(number_of_tweets):\n",
    "        #     if stuff.in_reply_to_status_id_str == status.id:\n",
    "        #         tweets.append(stuff)\n",
    "    return tweets\n",
    "\n",
    "\n",
    "tiktok_newstweets=keyword_tweetsandreplies(extractor,\"https://www.businessinsider.com/tiktok-confirms-us-user-data-accessed-in-china-bytedance-2022-7\",2400)\n",
    "\n",
    "# create a panda DataFrame by looping through each element and add it to the DataFrame\n",
    "data = pd.DataFrame(data=[tweet.full_text for tweet in tiktok_newstweets], \n",
    "                    columns=['Tweets'])\n",
    "data['Tweets_ID'] = [tweet.id for tweet in tiktok_newstweets]\n",
    "data['Date'] = [tweet.created_at for tweet in tiktok_newstweets]\n",
    "data['Source'] = [tweet.source for tweet in tiktok_newstweets]\n",
    "data['Likes_no'] = [tweet.favorite_count for tweet in tiktok_newstweets]\n",
    "data['Retweets_no'] = [tweet.retweet_count for tweet in tiktok_newstweets]\n",
    "\n",
    "data=data.drop_duplicates(subset=('Tweets'))\n",
    "\n",
    "data.to_csv(\"TikTokNewsTweets.csv\")\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words):\n",
    "    cleaned_tokens=[]\n",
    "    for token in tweet_tokens:\n",
    "        token = re.sub('http([!-~]+)?','',token)\n",
    "        token = re.sub('//t.co/[A-Za-z0-9]+','',token)\n",
    "        token = re.sub('(@[A-Za-z0-9_]+)','',token)\n",
    "        token = re.sub('[0-9]','',token)\n",
    "        token = re.sub('[^ -~]','',token)\n",
    "        token = re.sub(emoji.get_emoji_regexp(), \"\", token)\n",
    "        token = token.replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n",
    "        token = re.sub('[^\\x00-\\x7f]','', token) \n",
    "        token = re.sub(\"\\s\\s+\" , \" \", token)\n",
    "        if (len(token)>3) and (token not in string.punctuation) and (token.lower() not in stop_words):\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "\n",
    "# important to remove as many unnecessary words as possible\n",
    "# requires self explaratory and self understanding of problems and context\n",
    "stop_words=stopwords.words('english')\n",
    "tweets_token=data['Tweets'].apply(word_tokenize).tolist()\n",
    "\n",
    "cleaned_tokens=[]\n",
    "# lemmanise and remove noise function to help clean the data\n",
    "# at the end returns the list of 'cleaned tokens'\n",
    "for token in tweets_token:\n",
    "    rm_noise =remove_noise(token, stop_words)\n",
    "    # lemma_tokens=lemmatize_sentence(rm_noise)\n",
    "    # cleaned_tokens.append(lemma_tokens)\n",
    "    cleaned_tokens.append(rm_noise)\n",
    "\n",
    "tweet_list=[]\n",
    "for tokens in cleaned_tokens:\n",
    "    toke=' '. join([str(token) for token in tokens])\n",
    "    tweet_list.append(toke)\n",
    "\n",
    "data['CleanTweet']=tweet_list\n",
    "\n",
    "# data=data.drop_duplicates(subset=('CleanTweet'))\n",
    "twstring=''. join([str(item) for item in data['CleanTweet']])\n",
    "twstring = re.sub('(?:^|\\W)data(?:$|\\W)|(?:^|\\W)user(?:$|\\W)|(?:^|\\W)cleaned(?:$|\\W)|(?:^|\\W)confirms(?:$|\\W)|(?:^|\\W)confirm(?:$|\\W)','',twstring)\n",
    "# twstring =''. join([str(item) for item in tweet_list])\n",
    "tweets_wordcloud=WordCloud(width=400,height=400,\n",
    "                            background_color='white',\n",
    "                            min_font_size=10,collocations=False).generate(twstring)\n",
    "plt.imshow(tweets_wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "text_blob=[]\n",
    "for tweet in data['Tweets'].tolist():\n",
    "    analysis = TextBlob(tweet)\n",
    "    if analysis.sentiment.polarity==0:\n",
    "        sentiment=\"Neutral\"\n",
    "    elif analysis.sentiment.polarity>0:\n",
    "        sentiment=\"Positive\"\n",
    "    elif analysis.sentiment.polarity<0:\n",
    "        sentiment=\"Negative\"\n",
    "    text_blob.append(sentiment)\n",
    "    \n",
    "data[\"Sentiment\"]=text_blob\n",
    "\n",
    "data.groupby(by='Sentiment').mean()\n",
    "data.groupby(by='Sentiment')['Sentiment'].count()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
